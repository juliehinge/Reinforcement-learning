{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, reward is 100.40547127893348\n",
      "Episode 2, reward is 100.45180623057408\n",
      "Episode 3, reward is 100.173726320473\n",
      "Episode 4, reward is 100.43400565670605\n",
      "Episode 5, reward is 100.39180753361188\n",
      "Episode 6, reward is 100.40166179767633\n",
      "Episode 7, reward is 100.27196880633907\n",
      "Episode 8, reward is 100.46214751895275\n",
      "Episode 9, reward is 100.46291499815263\n",
      "Episode 10, reward is 100.36431569863798\n",
      "Episode 11, reward is 100.45317904560382\n",
      "Episode 12, reward is 100.39667690330413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m         finish_episode()  \u001b[38;5;66;03m# Perform policy update at the end of the episode\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print message when training completes\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Start training process\u001b[39;00m\n\u001b[1;32m    122\u001b[0m train()\n",
      "Cell \u001b[0;32mIn[41], line 115\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, reward is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i_episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, reward))  \u001b[38;5;66;03m# Print episode number and final reward\u001b[39;00m\n\u001b[1;32m    113\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mfinish_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform policy update at the end of the episode\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 90\u001b[0m, in \u001b[0;36mfinish_episode\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero out gradients\u001b[39;00m\n\u001b[1;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(policy_loss)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(value_loss)\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# Total loss\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagate the total loss\u001b[39;00m\n\u001b[1;32m     91\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(policy\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the optimizer parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as Adam\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.01  # Learning rate for the nn optimizer\n",
    "GAMMA = 0.997  # Discount factor\n",
    "NUM_EPISODES = 1000  # Number of training episodes\n",
    "eps = np.finfo(np.float32).eps.item()  # Small epsilon value to prevent division by zero\n",
    "\n",
    "num_state = env.observation_space.shape[0]  # Dimensionality of the environment space\n",
    "action_shape = env.action_space.shape[0]  # Dimensionality of the continuous environment space\n",
    "action_low = env.action_space.low  # Lower bound of actions\n",
    "action_high = env.action_space.high  # Upper bound of actions\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_state, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Dropout layer with a dropout probability of 0.5\n",
    "        self.mean = nn.Linear(64, action_shape)\n",
    "        self.std = nn.Linear(64, action_shape)  # Allows policy to explore within the normal distribution defined by the mean and std\n",
    "        self.expected_return = nn.Linear(64, 1)  # Estimate of the expected return by following the current policy\n",
    "        self.action_values = []  # Lists to store actions and rewards during training\n",
    "        self.rewards = []\n",
    "        self.gamma = GAMMA  # Discount factor for future rewards\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))  # Pass input through the first layer and apply ReLU activation\n",
    "        x = self.dropout(x)  # Apply dropout after the first layer\n",
    "        x = nn.functional.relu(self.fc2(x))  # Pass through the second layer and apply ReLU activation\n",
    "        x = self.dropout(x)  # Apply dropout after the second layer\n",
    "        mean = torch.tanh(self.mean(x))  # Output the mean of the action distribution (squashes the output to be between -1 and 1)\n",
    "        std = torch.exp(self.std(x))  # Output the standard deviation of the action distribution\n",
    "        expected_return = self.expected_return(x)  # Output the state value\n",
    "        return mean, std, expected_return\n",
    "\n",
    "# Initialize the policy network\n",
    "policy = Module()\n",
    "# Initialize the optimizer\n",
    "optimizer = Adam.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "# Named tuple to save actions and their log probabilities\n",
    "saveAction = namedtuple('SavedActions', ['log_prob', 'expected_return'])\n",
    "\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "\n",
    "    state = torch.from_numpy(state).float()  # Convert the state to a PyTorch tensor\n",
    "    mean, std, expected_return = policy(state)  # Pass the current state through the policy network and get mean, std, and expected return\n",
    "    c = Normal(mean, std)  # Create a normal distribution with the mean and standard deviation\n",
    "    action = c.sample()  # Sample an action from the normal distribution\n",
    "    log_prob = c.log_prob(action).sum(dim=-1)  # Calculate the log probability of the sampled action\n",
    "    policy.action_values.append(saveAction(log_prob, expected_return))  # Save the log probability and expected return\n",
    "    \n",
    "    return action.detach().numpy()  # Detach the action and convert it to a NumPy array\n",
    "\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    rewards = []\n",
    "    saved_actions = policy.action_values  # Get saved actions and their log probabilities\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    R = 0\n",
    "    for r in policy.rewards[::-1]:  # Iterate over rewards in reverse order\n",
    "        R = r + policy.gamma * R  # Apply discount factor gamma\n",
    "        rewards.insert(0, R)  # Insert discounted reward at the beginning of the list\n",
    "\n",
    "    rewards = torch.tensor(rewards)  # Convert rewards list to a PyTorch tensor\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)  # Normalize rewards\n",
    "    \n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    \n",
    "    # Calculate policy and expected return losses\n",
    "    for (log_prob, expected_return), reward in zip(saved_actions, rewards):\n",
    "        advantage = r - expected_return.item()  # Calculate advantage\n",
    "        policy_loss.append(-log_prob * advantage)  # Policy loss: negative log probability multiplied by advantage\n",
    "        value_loss.append(nn.functional.smooth_l1_loss(expected_return, torch.tensor([reward])))  # Value loss: smooth L1 loss between expected return and discounted reward\n",
    "    \n",
    "\n",
    "\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()  # Total loss\n",
    "    loss.backward()  # Backpropagate the total loss\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1)\n",
    "    optimizer.step()  # Update the optimizer parameters\n",
    "\n",
    "\n",
    "    del policy.rewards[:]  # Clear rewards list for the next episode\n",
    "    del policy.action_values[:]  # Clear action values list for the next episode\n",
    "\n",
    "\n",
    "def train():\n",
    "    run_steps = []\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        state, _ = env.reset()  # Reset environment and get initial state\n",
    "        for t in count():\n",
    "            action = select_action(state)  # Select an action based on the current state\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Take action and observe next state and reward\n",
    "            reward = next_state[0] + reward  # Modify reward if needed\n",
    "            policy.rewards.append(reward)  # Store reward in the policy\n",
    "            state = next_state  # Update current state to next state\n",
    "            \n",
    "            if done:\n",
    "                run_steps.append(t)  # Record number of steps taken in the episode\n",
    "                print(\"Episode {}, reward is {}\".format(i_episode + 1, reward))  # Print episode number and final reward\n",
    "                break\n",
    "        \n",
    "        finish_episode()  # Perform policy update at the end of the episode\n",
    "    \n",
    "    print(\"Training finished.\")  # Print message when training completes\n",
    "\n",
    "train()  # Start training process\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Normal\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.functional import smooth_l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dkjulhin/Library/Python/3.9/lib/python/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, run step is 61184\n",
      "Episode 2, run step is 24575\n",
      "Episode 3, run step is 9884\n",
      "Episode 4, run step is 6780\n",
      "Episode 5, run step is 5991\n",
      "Episode 6, run step is 564\n",
      "Episode 7, run step is 12592\n",
      "Episode 8, run step is 4333\n",
      "Episode 9, run step is 3375\n",
      "Episode 10, run step is 752\n",
      "Episode 11, run step is 1130\n",
      "Episode 12, run step is 8969\n",
      "Episode 13, run step is 8425\n",
      "Episode 14, run step is 2940\n",
      "Episode 15, run step is 3320\n",
      "Episode 16, run step is 1817\n",
      "Episode 17, run step is 3105\n",
      "Episode 18, run step is 8585\n",
      "Episode 19, run step is 1274\n",
      "Episode 20, run step is 2361\n",
      "Episode 21, run step is 1487\n",
      "Episode 22, run step is 11516\n",
      "Episode 23, run step is 12488\n",
      "Episode 24, run step is 2889\n",
      "Episode 25, run step is 3253\n",
      "Episode 26, run step is 3035\n",
      "Episode 27, run step is 9535\n",
      "Episode 28, run step is 5835\n",
      "Episode 29, run step is 15447\n",
      "Episode 30, run step is 1355\n",
      "Episode 31, run step is 3465\n",
      "Episode 32, run step is 1561\n",
      "Episode 33, run step is 19556\n",
      "Episode 34, run step is 9103\n",
      "Episode 35, run step is 9441\n",
      "Episode 36, run step is 1648\n",
      "Episode 37, run step is 10920\n",
      "Episode 38, run step is 3628\n",
      "Episode 39, run step is 4898\n",
      "Episode 40, run step is 14031\n",
      "Episode 41, run step is 12494\n",
      "Episode 42, run step is 15932\n",
      "Episode 43, run step is 2766\n",
      "Episode 44, run step is 7509\n",
      "Episode 45, run step is 3399\n",
      "Episode 46, run step is 4711\n",
      "Episode 47, run step is 7705\n",
      "Episode 48, run step is 2957\n",
      "Episode 49, run step is 1166\n",
      "Episode 50, run step is 8073\n",
      "Episode 51, run step is 4711\n",
      "Episode 52, run step is 11549\n",
      "Episode 53, run step is 4165\n",
      "Episode 54, run step is 4836\n",
      "Episode 55, run step is 6793\n",
      "Episode 56, run step is 6514\n",
      "Episode 57, run step is 5390\n",
      "Episode 58, run step is 2956\n",
      "Episode 59, run step is 5025\n",
      "Episode 60, run step is 1552\n",
      "Episode 61, run step is 7253\n",
      "Episode 62, run step is 3112\n",
      "Episode 63, run step is 28300\n",
      "Episode 64, run step is 3432\n",
      "Episode 65, run step is 7901\n",
      "Episode 66, run step is 10987\n",
      "Episode 67, run step is 5646\n",
      "Episode 68, run step is 3329\n",
      "Episode 69, run step is 65273\n",
      "Episode 70, run step is 20952\n",
      "Episode 71, run step is 22218\n",
      "Episode 72, run step is 19840\n",
      "Episode 73, run step is 14646\n",
      "Episode 74, run step is 33944\n",
      "Episode 75, run step is 54381\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 107\u001b[0m\n\u001b[1;32m    103\u001b[0m         finish_episode()  \u001b[38;5;66;03m# Perform policy update at the end of the episode\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print message when training completes\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Start training process\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 103\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, run step is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i_episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Print episode number and number of steps taken\u001b[39;00m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mfinish_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform policy update at the end of the episode\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 81\u001b[0m, in \u001b[0;36mfinish_episode\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero out gradients\u001b[39;00m\n\u001b[1;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(policy_loss)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(expected_return_loss)\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# Total loss\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagate the total loss\u001b[39;00m\n\u001b[1;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the optimizer parameters\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m policy\u001b[38;5;241m.\u001b[39mrewards[:]  \u001b[38;5;66;03m# Clear rewards list for the next episode\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as Adam\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.01  # Learning rate for the nn optimizer\n",
    "GAMMA = 0.995  # Discount factor\n",
    "NUM_EPISODES = 1000  # Number of training episodes\n",
    "eps = np.finfo(np.float32).eps.item()  # Small epsilon value to prevent division by zero\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "num_state = env.observation_space.shape[0]  # Dimensionality of the environment space\n",
    "action_shape = env.action_space.shape[0]  # Dimensionality of the continuous environment space\n",
    "action_low = env.action_space.low  # Lower bound of actions\n",
    "action_high = env.action_space.high  # Upper bound of actions\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_state, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.mean_head = nn.Linear(64, action_shape)\n",
    "        self.std_head = nn.Linear(64, action_shape)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "        self.policy_action_value = []  # Lists to store actions and rewards during training\n",
    "        self.rewards = []\n",
    "        self.gamma = GAMMA  # Discount factor for future rewards\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))  # Pass input through the first layer and apply ReLU activation\n",
    "        x = nn.functional.relu(self.fc2(x))  # Pass through the second layer and apply ReLU activation\n",
    "        mean = torch.tanh(self.mean_head(x))  # Output the mean of the action distribution (squashes the output to be between -1 and 1)\n",
    "        std = torch.exp(self.std_head(x))  # Output the standard deviation of the action distribution\n",
    "        expected_return = self.value_head(x)  # Output the state expected return\n",
    "        return mean, std, expected_return\n",
    "\n",
    "# Initialize the policy network\n",
    "policy = Module()\n",
    "# Initialize the optimizer\n",
    "optimizer = Adam.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "# Named tuple to save actions and their log probabilities\n",
    "saveAction = namedtuple('SavedActions', ['log_prob', 'expected_return'])\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()  # Convert the state to a PyTorch tensor\n",
    "    mean, std, expected_return = policy(state)  # Pass the current state through the policy network and get mean, std, and expected return\n",
    "    c = Normal(mean, std)  # Create a normal distribution with the mean and standard deviation\n",
    "    action = c.sample()  # Sample an action from the normal distribution\n",
    "    log_prob = c.log_prob(action).sum(dim=-1)  # Calculate the log probability of the sampled action\n",
    "    policy.policy_action_value.append(saveAction(log_prob, expected_return))  # Save the log probability and value\n",
    "    \n",
    "    return action.detach().numpy()  # Detach the action and convert it to a NumPy array\n",
    "\n",
    "def finish_episode():\n",
    "    rewards = []\n",
    "    saved_actions = policy.policy_action_value  # Get saved actions and their log probabilities\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    R = 0\n",
    "    for r in policy.rewards[::-1]:  # Iterate over rewards in reverse order\n",
    "        R = r + policy.gamma * R  # Apply discount factor gamma\n",
    "        rewards.insert(0, R)  # Insert discounted reward at the beginning of the list\n",
    "\n",
    "    rewards = torch.tensor(rewards)  # Convert rewards list to a PyTorch tensor\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)  # Normalize rewards\n",
    "    \n",
    "    policy_loss = []\n",
    "    expected_return_loss = []\n",
    "    \n",
    "    # Calculate policy and value losses\n",
    "    for (log_prob, expected_return), reward in zip(saved_actions, rewards):\n",
    "        advantage = reward - expected_return.item()  # Calculate advantage\n",
    "        policy_loss.append(-log_prob * advantage)  # Policy loss: negative log probability multiplied by advantage\n",
    "        expected_return_loss.append(nn.functional.smooth_l1_loss(expected_return, torch.tensor([reward])))  \n",
    "\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "    loss = torch.stack(policy_loss).sum() + torch.stack(expected_return_loss).sum()  # Total loss\n",
    "    loss.backward()  # Backpropagate the total loss\n",
    "    optimizer.step()  # Update the optimizer parameters\n",
    "\n",
    "    del policy.rewards[:]  # Clear rewards list for the next episode\n",
    "    del policy.policy_action_value[:]  # Clear action values list for the next episode\n",
    "\n",
    "def train():\n",
    "    run_steps = []\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        state, _ = env.reset()  # Reset environment and get initial state\n",
    "        for t in count():\n",
    "            action = select_action(state)  # Select an action based on the current state\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Take action and observe next state and reward\n",
    "            reward = next_state[0] + reward  # Modify reward if needed\n",
    "            policy.rewards.append(reward)  # Store reward in the policy\n",
    "            state = next_state  # Update current state to next state\n",
    "            \n",
    "            if done:\n",
    "                run_steps.append(t)  # Record number of steps taken in the episode\n",
    "                print(\"Episode {}, run step is {}\".format(i_episode + 1, t + 1))  # Print episode number and number of steps taken\n",
    "                break\n",
    "        \n",
    "        finish_episode()  # Perform policy update at the end of the episode\n",
    "    \n",
    "    print(\"Training finished.\")  # Print message when training completes\n",
    "\n",
    "train()  # Start training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob\n",
    "import base64, io, os\n",
    "\n",
    "\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def visualize_model(render_mode, max_steps=10000, seed=42):\n",
    "    # Initialize the environment with the given render mode\n",
    "    env = gym.make(\"MountainCarContinuous-v0\", render_mode=render_mode)\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    state, info = env.reset(seed=seed)\n",
    "    \n",
    "    for episode in range(1):\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not (terminated or truncated) and steps < max_steps:\n",
    "            # Choose an action using the agent\n",
    "            action = select_action(state)\n",
    "            # Take the action and get the next state and reward\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            state = new_state\n",
    "            \n",
    "            # Render the environment if in rgb_array mode for animation\n",
    "            if render_mode == 'rgb_array':\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "            steps += 1\n",
    "        \n",
    "        # Reset the environment for the next episode\n",
    "        state, info = env.reset()\n",
    "    \n",
    "    # Close the environment\n",
    "    env.close()\n",
    "\n",
    "visualize_model(render_mode='rgb_array')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
